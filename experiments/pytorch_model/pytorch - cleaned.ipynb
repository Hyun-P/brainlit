{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bb4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import scipy.ndimage as ndi\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import dice\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74dc6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ea5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading segments to /users/shrey2/Documents/NDD/brainlit/docs/notebooks/utils/data\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/users/shrey2/Documents/NDD/brainlit/docs/notebooks/utils/data\"\n",
    "print(f\"Downloading segments to {data_dir}\")\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "im_dir = Path(os.path.join(data_dir, \"sample-tif-location\"))\n",
    "if not os.path.exists(im_dir):\n",
    "    os.makedirs(im_dir)\n",
    "\n",
    "swc_dir = os.path.join(data_dir, \"sample-swc-location\")\n",
    "if not os.path.exists(swc_dir):\n",
    "    os.makedirs(swc_dir)\n",
    "\n",
    "mask_dir = Path(os.path.join(data_dir, \"mask-location\"))\n",
    "\n",
    "swc_base_path = Path(swc_dir) / \"Manual-GT\"\n",
    "\n",
    "gfp_files = list(im_dir.glob(\"**/*-gfp.tif\"))\n",
    "\n",
    "ms = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8911c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fc0db9bc2942e587eaf4c4adb40812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_torch = []\n",
    "y_torch = []\n",
    "\n",
    "for i, im_path in enumerate(tqdm(gfp_files)):\n",
    "\n",
    "    f = im_path.parts[-1][:-8].split(\"_\")\n",
    "    image = f[0]\n",
    "    num = int(f[1])\n",
    "\n",
    "    if (image == \"test\" and num in [9,10,24]) or (image == \"validation\" and num in [11]):\n",
    "        continue\n",
    "\n",
    "    #getting image\n",
    "    im = io.imread(im_path, plugin=\"tifffile\")\n",
    "    im = (im - np.amin(im)) / (np.amax(im) - np.amin(im))\n",
    "    im = np.swapaxes(im,0,2)\n",
    "    im_padded = np.pad(im, ((4,4), (4,4), (3,3)) )\n",
    "    \n",
    "    #getting ground truth mask\n",
    "    file_name = str(im_path)[str(im_path).find(\"\\\\\", 80) + 1 : (str(im_path).find(\"sample\"))] + \"/mask-location/\"\n",
    "    file_num = file_name[file_name.find(\"_\")+1:]\n",
    "    if file_name[0] == 'v':\n",
    "        file_num = str(int(file_num)+25)\n",
    "    mask_path = Path(file_name + f[0] + \"_\" + f[1] + \"_mask.npy\")\n",
    "    mask = np.load(mask_path)\n",
    "    \n",
    "    X_torch.append(im)\n",
    "    y_torch.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07636b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = np.reshape(X_torch, (1, 46, 330, 330, 100)) #making it 5 channels for the 3d cnn\n",
    "y_torch = np.reshape(y_torch, (1, 46, 330, 330, 100))\n",
    "\n",
    "X_torch_train = X_torch[:, 0:23] #50/50 train test split\n",
    "y_torch_train = y_torch[:, 0:23]\n",
    "x_torch_test = X_torch[:, 23:46]\n",
    "y_torch_test = y_torch[:, 23:46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c8a35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.tensor([X_torch_train, y_torch_train]).float()\n",
    "test_data = torch.tensor([x_torch_test, y_torch_test]).float()\n",
    "\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a21dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34e020d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([1, 23, 330, 330, 100])\n",
      "Labels batch shape: torch.Size([1, 23, 330, 330, 100])\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d9b14",
   "metadata": {},
   "source": [
    "## Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d35441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Conv3d(1, 1, kernel_size = 3, stride = 1, padding = 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        logits = self.Sigmoid(logits)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb0162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch, (X_all, y_all) in enumerate(dataloader):\n",
    "        \n",
    "        loss_list = []\n",
    "\n",
    "        for image in range(X_all.shape[1]):\n",
    "            X = np.reshape(X_all[0][image], (1, 1, 330, 330, 100))\n",
    "            y = np.reshape(y_all[0][image], (1, 1, 330, 330, 100))\n",
    "            \n",
    "            # Compute prediction and loss\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            pred = torch.squeeze(pred, 3).clone()\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            loss_list.append(loss)\n",
    "            \n",
    "        print(\"Avg loss:\", np.average(loss_list))\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, print_acc):\n",
    "    size = 330*330*100*14\n",
    "    print(\"Size: \", size)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    for batch, (X_all, y_all) in enumerate(dataloader):\n",
    "\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_list = []\n",
    "        x_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image in range(X_all.shape[1]):\n",
    "                X = np.reshape(X_all[0][image], (1, 1, 330, 330, 100))\n",
    "                y = np.reshape(y_all[0][image], (1, 1, 330, 330, 100))\n",
    "                pred = model(X)\n",
    "                pred = torch.squeeze(pred, 3)\n",
    "                \n",
    "                x_list.append(X)\n",
    "                y_list.append(y)\n",
    "                y_pred.append(pred)\n",
    "                \n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "    \n",
    "    #test acc\n",
    "  #  if print_acc == True:\n",
    "  #      acc_list = []\n",
    "  #      i = 0\n",
    "  #      while i < len(y_pred):\n",
    "  #          pred_r = np.round(y_pred[i].detach().numpy()).flatten()\n",
    "  #          target = np.array(y_list[i]).astype(int).flatten()\n",
    "  #          acc = accuracy_score(pred_r, target)\n",
    "  #          acc_list.append(acc)\n",
    "        \n",
    "        #pred_r = np.round(y_pred.flatten().detach())\n",
    "        #target = np.array(y_list).flatten().astype(int)\n",
    "        #pred_r = np.round(pred.detach().numpy()).flatten()\n",
    "        #target = np.array(y).astype(int).flatten()\n",
    "        #acc = accuracy_score(pred_r, target)\n",
    "  #      print(\"Test accuracy: \", np.mean(acc_list))\n",
    "    \n",
    "    return x_list, y_pred, y_list, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8011318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe289f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e990e642849347de8e8b10c1c1a4cecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Avg loss: 0.9937952756881714\n",
      "Size:  152460000\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Avg loss: 0.993795273096665\n",
      "Size:  152460000\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Avg loss: 0.993795273096665\n",
      "Size:  152460000\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Avg loss: 0.9937952705051588\n",
      "Size:  152460000\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Avg loss: 0.9937952705051588\n",
      "Size:  152460000\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Avg loss: 0.9937952679136525\n",
      "Size:  152460000\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Avg loss: 0.9937952679136525\n",
      "Size:  152460000\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Avg loss: 0.9937952627306399\n",
      "Size:  152460000\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Avg loss: 0.9937952601391337\n",
      "Size:  152460000\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Avg loss: 0.9937952601391337\n",
      "Size:  152460000\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#weights = torch.tensor([.75, .25])\n",
    "#from torchgeometry.losses.dice import DiceLoss\n",
    "\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn = DiceLoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "\n",
    "pred_list = []\n",
    "y_list = []\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "for t in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    if (epochs % 5 == 0):\n",
    "        print_acc = True\n",
    "    else:\n",
    "        print_acc = False\n",
    "    x_list, y_pred, y, loss = test_loop(test_dataloader, model, loss_fn, print_acc)\n",
    "    pred_list.append(y_pred)\n",
    "    y_list.append(y)\n",
    "    loss_list.append(loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9631bd3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc758854248246ff82d3af2d4b6525b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880ecf46403b471e813daa868e7adb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57571c73d33e44a2b1f07e26c0b4b110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e956f5ed7684d6b89c89644b3a73b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3214256e497044e9be8bfa4ed63a108d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a3712ed83846c28916987d14d58b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c476386e1b041a98e96eb5d68bfca40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56b9d208dbf43cfb52c892e95e3e900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd40dd65c784609a9dcafb547cca3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc433f2f9ee41ad98eeb54ca1389083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52ed3109206461b9e7376813e87cd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# getting accuracy, precision, recall at each epoch\n",
    "\n",
    "acc_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "\n",
    "for i in tqdm(range(len(pred_list))):\n",
    "    acc_list_t = []\n",
    "    precision_list_t = []\n",
    "    recall_list_t = []\n",
    "    \n",
    "    for j in tqdm(range(len(pred_list[0]))):\n",
    "        pred = pred_list[i][j].clone().numpy()[:, 0].round().astype(int).flatten()\n",
    "        target = y_list[i][j][:, 0].clone().numpy().astype(int).flatten()\n",
    "\n",
    "        acc = accuracy_score(target, pred) * 100\n",
    "        acc_list_t.append(acc)\n",
    "\n",
    "        pr = precision_score(target, pred) * 100\n",
    "        precision_list_t.append(pr)\n",
    "\n",
    "        rc = recall_score(target, pred) * 100\n",
    "        recall_list_t.append(rc)\n",
    "        \n",
    "    mean_acc = np.mean(acc_list_t)\n",
    "    mean_pr = np.mean(precision_list_t)\n",
    "    mean_rc = np.mean(recall_list_t)\n",
    "    \n",
    "    acc_list.append(mean_acc)\n",
    "    precision_list.append(mean_pr)\n",
    "    recall_list.append(mean_rc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test stats\n",
    "\n",
    "def get_stats(stat, epoch, acc_list, precision_list, recall_list):\n",
    "    if stat == \"accuracy\":\n",
    "        print(\"Accuracy at epoch \" + str(epoch) + \" is \" + str(acc_list[epoch - 1]))\n",
    "    if stat == \"all\":\n",
    "        print(\"Accuracy at epoch \" + str(epoch) + \" is \" + str(acc_list[epoch - 1]))\n",
    "        print(\"Precision at epoch \" + str(epoch) + \" is \" + str(precision_list[epoch - 1]))\n",
    "        print(\"Recall at epoch \" + str(epoch) + \" is \" + str(recall_list[epoch - 1]))\n",
    "        \n",
    "get_stats(\"all\", 2, acc_list, precision_list, recall_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df57fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Test loss over epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Test loss\")\n",
    "plt.plot(loss_list)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Accuracy over epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg accuracy (%)\")\n",
    "plt.plot(acc_list)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Precision over epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg precision (%)\")\n",
    "plt.plot(precision_list)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Recall over epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg recall (%)\")\n",
    "plt.plot(recall_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7321ec5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Accuracy histogram on last epoch\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Accuracy histogram for individual 23 images on last epoch\")\n",
    "plt.ylabel(\"Individual Accuracy\")\n",
    "plt.hist(acc_list_t, bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6386ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot all images/predictions from last epoch\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "for i in range(len(y_list[len(y_list) - 1])):\n",
    "    x = x_list[i].clone()[:,0].numpy()\n",
    "    pred = pred_list[len(pred_list) - 1][i].clone()[:,0].numpy()\n",
    "    y = y_list[len(y_list) - 1][i].clone()[:,0].numpy()\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y.flatten(), pred.flatten())\n",
    "    optimal_thresh = thresholds[np.argmax(tpr - fpr)]\n",
    "    print(\"Optimal Threshold for image \" + str(i) + \": \", optimal_thresh)\n",
    "    \n",
    "    pred_thresh = pred\n",
    "    \n",
    "    for i in range(1):\n",
    "        for a in range(330):\n",
    "            for b in range(330):\n",
    "                for c in range(100):\n",
    "                    if pred[i][a][b][c]  > optimal_thresh:\n",
    "                        pred_thresh[i][a][b][c] = 1\n",
    "                    else:\n",
    "                        pred_thresh[i][a][b][c] = 0\n",
    "\n",
    "\n",
    "    import napari\n",
    "    with napari.gui_qt():\n",
    "        viewer = napari.Viewer(ndisplay=3)\n",
    "        viewer.add_image(x[0])\n",
    "        viewer.add_labels(y[0].astype(int))\n",
    "        viewer.add_labels(pred_thresh[0].astype(int), num_colors = 2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2a9e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics for one image\n",
    "\n",
    "#pred_0 = pred.clone().numpy()[:, 0].round().astype(int).flatten()\n",
    "#target_0 = y_torch_test[:, 0].astype(int).flatten()\n",
    "\n",
    "#print(\"Accuracy: \", accuracy_score(target_0, pred_0) * 100)\n",
    "#print(\"Precision: \", precision_score(target_0, pred_0) * 100)\n",
    "#print(\"Recall: \", recall_score(target_0, pred_0) * 100)\n",
    "#print(\"Nonzeros: \", np.count_nonzero(pred_0) * 100)\n",
    "#print(\"Nonzeros percent: \", np.count_nonzero(pred_0) / len(target_0) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fa7478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc\\nimport matplotlib.pyplot as plt\\n\\n\\nlast_pred = pred_list[len(pred_list)-1][len(pred_list[0])-1].clone()\\n#last_pred = pred_list[len(pred_list)-2].clone()\\ny_torch_test_0 = y_torch_test[:, 0]\\n\\nfpr, tpr, thresholds = roc_curve(y_torch_test_0.flatten(), last_pred.flatten())\\nroc_auc = auc(fpr, tpr)\\n\\nprecision, recall, thresholds = precision_recall_curve(y_torch_test_0.flatten(), last_pred.flatten())\\n\\n#printing optimal ROC threshold\\noptimal_thresh = thresholds[np.argmax(tpr - fpr)]\\nprint(\"Optimal Threshold: \", optimal_thresh)\\n\\n#plotting\\nplt.figure()\\nplt.title(\"ROC\")\\nplt.xlabel(\"fpr\")\\nplt.ylabel(\"tpr\")\\nplt.plot(fpr, tpr, label = \\'AUC = %0.2f\\' % roc_auc)\\nplt.legend(loc = \\'lower right\\')\\n\\nplt.figure()\\nplt.title(\"PR\")\\nplt.xlabel(\"recall\")\\nplt.ylabel(\"precision\")\\nplt.plot(recall, precision, label = \"AP = %0.2f\" % np.mean(precision))\\nplt.legend(loc = \\'lower right\\')\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PR and ROC Curves\n",
    "'''\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "last_pred = pred_list[len(pred_list)-1][len(pred_list[0])-1].clone()\n",
    "#last_pred = pred_list[len(pred_list)-2].clone()\n",
    "y_torch_test_0 = y_torch_test[:, 0]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_torch_test_0.flatten(), last_pred.flatten())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_torch_test_0.flatten(), last_pred.flatten())\n",
    "\n",
    "#printing optimal ROC threshold\n",
    "optimal_thresh = thresholds[np.argmax(tpr - fpr)]\n",
    "print(\"Optimal Threshold: \", optimal_thresh)\n",
    "\n",
    "#plotting\n",
    "plt.figure()\n",
    "plt.title(\"ROC\")\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"PR\")\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.plot(recall, precision, label = \"AP = %0.2f\" % np.mean(precision))\n",
    "plt.legend(loc = 'lower right')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7d1464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npred2_0 = last_pred[:,0]\\n\\npredN0 = pred2_0.numpy()\\n#for i in range(predN.shape[0]):\\nfor i in range(1):\\n    for a in range(330):\\n        for b in range(330):\\n            for c in range(100):\\n                if predN0[i][a][b][c]  > optimal_thresh:\\n                    predN0[i][a][b][c] = 1\\n                else:\\n                    predN0[i][a][b][c] = 0\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#thresholding for test image 0\n",
    "\n",
    "#pred2 = pred_list[len(pred_list)-2].clone().detach()\n",
    "\n",
    "#pred2 = pred.clone().detach()\n",
    "#pred2_0 = pred2[:,0]\n",
    "'''\n",
    "pred2_0 = last_pred[:,0]\n",
    "\n",
    "predN0 = pred2_0.numpy()\n",
    "#for i in range(predN.shape[0]):\n",
    "for i in range(1):\n",
    "    for a in range(330):\n",
    "        for b in range(330):\n",
    "            for c in range(100):\n",
    "                if predN0[i][a][b][c]  > optimal_thresh:\n",
    "                    predN0[i][a][b][c] = 1\n",
    "                else:\n",
    "                    predN0[i][a][b][c] = 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bfc7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import napari\\nwith napari.gui_qt():\\n    viewer = napari.Viewer(ndisplay=3)\\n    viewer.add_image((x_torch_test[0][22]))\\n    viewer.add_labels((y_torch_test[0][22]).astype(int))\\n    viewer.add_labels(predN0[0].astype(int), num_colors = 2)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import napari\n",
    "with napari.gui_qt():\n",
    "    viewer = napari.Viewer(ndisplay=3)\n",
    "    viewer.add_image((x_torch_test[0][22]))\n",
    "    viewer.add_labels((y_torch_test[0][22]).astype(int))\n",
    "    viewer.add_labels(predN0[0].astype(int), num_colors = 2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6852cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
