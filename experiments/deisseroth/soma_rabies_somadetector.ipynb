{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "from skimage.transform import downscale_local_mean\n",
    "import napari\n",
    "from skimage import io\n",
    "import random\n",
    "import h5py\n",
    "from skimage import measure\n",
    "from brainlit.preprocessing import removeSmallCCs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import tables\n",
    "from napari_animation import AnimationWidget\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from parse_ara import *\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import brainrender\n",
    "import scipy.ndimage as ndi\n",
    "from skimage.morphology import skeletonize\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from soma_rabies_somadetector_data import brain2paths, brain2centers\n",
    "from util import json_to_points\n",
    "\n",
    "%gui qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = \"8608\"\n",
    "channel = \"3channel\"\n",
    "dir = brain2paths[brain][\"ab\"]\n",
    "vol_fg = CloudVolume(dir, parallel=1, mip=0, fill_missing=False)\n",
    "print(vol_fg.shape)\n",
    "dir = brain2paths[brain][\"bg\"]\n",
    "vol_bg = CloudVolume(dir, parallel=1, mip=0, fill_missing=False)\n",
    "print(vol_bg.shape)\n",
    "dir = brain2paths[brain][\"endo\"]\n",
    "vol_endo = CloudVolume(dir, parallel=1, mip=0, fill_missing=False)\n",
    "print(vol_endo.shape)\n",
    "\n",
    "if \"vizlink\" in brain2paths[brain]:\n",
    "    url = brain2paths[brain][\"vizlink\"]\n",
    "    dict = json_to_points(url)\n",
    "    soma_centers = dict[\"soma_val\"]\n",
    "    nonsoma_centers = dict[\"nonsoma_val\"]\n",
    "elif brain in brain2centers.keys():\n",
    "    soma_centers = brain2centers[brain][0][0]\n",
    "    nonsoma_centers = brain2centers[brain][1][0]\n",
    "else:\n",
    "    print(\"No training/validation points\")\n",
    "\n",
    "print(f\"{len(soma_centers)} soma centers\")\n",
    "print(f\"{len(nonsoma_centers)} nonsoma centers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(new_array, scale=scale)\n",
    "viewer.scale_bar.visible = True\n",
    "viewer.scale_bar.unit = \"um\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = \"test\"\n",
    "\n",
    "type = \"pos\"\n",
    "for i, center in enumerate(soma_centers):\n",
    "    print(f\"Saving center: {center}\")\n",
    "    image_fg = vol_fg[\n",
    "        center[0] - 24 : center[0] + 25,\n",
    "        center[1] - 24 : center[1] + 25,\n",
    "        center[2] - 24 : center[2] + 25,\n",
    "    ]\n",
    "    image_fg = image_fg[:, :, :, 0]\n",
    "    image_bg = vol_bg[\n",
    "        center[0] - 24 : center[0] + 25,\n",
    "        center[1] - 24 : center[1] + 25,\n",
    "        center[2] - 24 : center[2] + 25,\n",
    "    ]\n",
    "    image_bg = image_bg[:, :, :, 0]\n",
    "    image_endo = vol_endo[\n",
    "        center[0] - 24 : center[0] + 25,\n",
    "        center[1] - 24 : center[1] + 25,\n",
    "        center[2] - 24 : center[2] + 25,\n",
    "    ]\n",
    "    image_endo = image_endo[:, :, :, 0]\n",
    "\n",
    "    image = np.squeeze(np.stack([image_fg, image_bg, image_endo], axis=0))\n",
    "\n",
    "    fname = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brain{brain}/{channel}/{dset_name}/{int(center[0])}_{int(center[1])}_{int(center[2])}_{type}.h5\"\n",
    "    with h5py.File(fname, \"w\") as f:\n",
    "        dset = f.create_dataset(\"image_3channel\", data=image)\n",
    "\n",
    "\n",
    "type = \"neg\"\n",
    "for i, center in enumerate(nonsoma_centers):\n",
    "    image_fg = vol_fg[\n",
    "        center[0] - 24 : center[0] + 25,\n",
    "        center[1] - 24 : center[1] + 25,\n",
    "        center[2] - 24 : center[2] + 25,\n",
    "    ]\n",
    "    image_fg = image_fg[:, :, :, 0]\n",
    "    image_bg = vol_bg[\n",
    "        center[0] - 24 : center[0] + 25,\n",
    "        center[1] - 24 : center[1] + 25,\n",
    "        center[2] - 24 : center[2] + 25,\n",
    "    ]\n",
    "    image_bg = image_bg[:, :, :, 0]\n",
    "    image_endo = vol_endo[\n",
    "        center[0] - 24 : center[0] + 25,\n",
    "        center[1] - 24 : center[1] + 25,\n",
    "        center[2] - 24 : center[2] + 25,\n",
    "    ]\n",
    "    image_endo = image_endo[:, :, :, 0]\n",
    "\n",
    "    image = np.squeeze(np.stack([image_fg, image_bg, image_endo], axis=0))\n",
    "\n",
    "    fname = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brain{brain}/{channel}/{dset_name}/{int(center[0])}_{int(center[1])}_{int(center[2])}_{type}.h5\"\n",
    "    with h5py.File(fname, \"w\") as f:\n",
    "        dset = f.create_dataset(\"image_3channel\", data=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubles = [\"3972_1636_1575_pos_Probabilities.h5\", \"2867_4336_1296_pos_Probabilities.h5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "files_dir = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brain{brain}/{channel}/test/\"\n",
    "onlyfiles = [f for f in listdir(files_dir) if isfile(join(files_dir, f))]\n",
    "# test_files = [f for f in onlyfiles if f[:4] == \"test\"]\n",
    "test_files = [f for f in onlyfiles if \"Probabilities\" in f]  # \"probabilities\"\n",
    "print(test_files)\n",
    "\n",
    "size_thresh = 500\n",
    "\n",
    "thresholds = list(np.arange(0.0, 1.0, 0.02))\n",
    "\n",
    "for threshold in thresholds:\n",
    "    tot_pos = 0\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    for filename in tqdm(test_files, disable=True):\n",
    "        if filename in doubles:\n",
    "            newpos = 2\n",
    "        else:\n",
    "            newpos = 1\n",
    "\n",
    "        fname = files_dir + filename\n",
    "        f = h5py.File(fname, \"r\")\n",
    "        pred = f.get(\"exported_data\")\n",
    "        pred = pred[0, :, :, :]\n",
    "        mask = pred > threshold\n",
    "        labels = measure.label(mask)\n",
    "        props = measure.regionprops(labels)\n",
    "\n",
    "        if \"pos\" in filename:\n",
    "            num_detected = 0\n",
    "            tot_pos += newpos\n",
    "            for prop in props:\n",
    "                if prop[\"area\"] > size_thresh:\n",
    "                    if num_detected < newpos:\n",
    "                        true_pos += 1\n",
    "                        num_detected += 1\n",
    "                    else:\n",
    "                        false_pos += 1\n",
    "        elif \"neg\" in filename:\n",
    "            for prop in props:\n",
    "                if prop[\"area\"] > size_thresh:\n",
    "                    false_pos += 1\n",
    "\n",
    "    recall = true_pos / tot_pos\n",
    "    recalls.append(recall)\n",
    "    if true_pos + false_pos == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "    precisions.append(precision)\n",
    "    if precision == 0 and recall == 0:\n",
    "        fscore = 0\n",
    "    else:\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    print(\n",
    "        f\"threshold: {threshold}: precision: {precision}, recall: {recall}, f-score: {fscore} for {tot_pos} positive samples in {len(test_files)} images\"\n",
    "    )\n",
    "\n",
    "fscores = [\n",
    "    2 * precision * recall / (precision + recall)\n",
    "    if (precision != 0 and recall != 0)\n",
    "    else 0\n",
    "    for precision, recall in zip(precisions, recalls)\n",
    "]\n",
    "dict = {\n",
    "    \"Recall\": recalls,\n",
    "    \"Precision\": precisions,\n",
    "    \"F-score\": fscores,\n",
    "    \"Threshold\": thresholds,\n",
    "}\n",
    "df = pd.DataFrame(dict)\n",
    "max_fscore = df[\"F-score\"].max()\n",
    "best_threshold = float(df.loc[df[\"F-score\"] == max_fscore][\"Threshold\"].iloc[0])\n",
    "best_rec = float(df.loc[df[\"F-score\"] == max_fscore][\"Recall\"].iloc[0])\n",
    "best_prec = float(df.loc[df[\"F-score\"] == max_fscore][\"Precision\"].iloc[0])\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.lineplot(data=df, x=\"Recall\", y=\"Precision\", estimator=np.amax, ci=False)\n",
    "plt.scatter(\n",
    "    best_rec,\n",
    "    best_prec,\n",
    "    c=\"r\",\n",
    "    label=f\"Max f-score: {max_fscore:.2f} thresh:{best_threshold:.2f}\",\n",
    ")\n",
    "plt.xlim([0, 1.1])\n",
    "plt.ylim([0, 1.1])\n",
    "plt.title(f\"Brain {brain} Validation: {tot_pos}+ {len(nonsoma_centers)}-\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All validation for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = \"3channel\"\n",
    "brains = [\"r1\", \"r2\", \"8607\", \"8606\", \"8477\", \"8531\", \"8608\"]\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "brain_ids = []\n",
    "\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_fscores = {}\n",
    "\n",
    "for brain in brains:\n",
    "\n",
    "    files_dir = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brain{brain}/{channel}/test/\"\n",
    "    onlyfiles = [f for f in listdir(files_dir) if isfile(join(files_dir, f))]\n",
    "    # test_files = [f for f in onlyfiles if f[:4] == \"test\"]\n",
    "    test_files = [f for f in onlyfiles if \"Probabilities\" in f]  # \"probabilities\"\n",
    "    print(test_files)\n",
    "\n",
    "    best_fscore = 0\n",
    "\n",
    "    size_thresh = 500\n",
    "\n",
    "    thresholds = list(np.arange(0.0, 1.0, 0.02))\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        tot_pos = 0\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        for filename in tqdm(test_files, disable=True):\n",
    "            if filename in doubles:\n",
    "                newpos = 2\n",
    "            else:\n",
    "                newpos = 1\n",
    "\n",
    "            fname = files_dir + filename\n",
    "            f = h5py.File(fname, \"r\")\n",
    "            pred = f.get(\"exported_data\")\n",
    "            pred = pred[0, :, :, :]\n",
    "            mask = pred > threshold\n",
    "            labels = measure.label(mask)\n",
    "            props = measure.regionprops(labels)\n",
    "\n",
    "            if \"pos\" in filename:\n",
    "                num_detected = 0\n",
    "                tot_pos += newpos\n",
    "                for prop in props:\n",
    "                    if prop[\"area\"] > size_thresh:\n",
    "                        if num_detected < newpos:\n",
    "                            true_pos += 1\n",
    "                            num_detected += 1\n",
    "                        else:\n",
    "                            false_pos += 1\n",
    "            elif \"neg\" in filename:\n",
    "                for prop in props:\n",
    "                    if prop[\"area\"] > size_thresh:\n",
    "                        false_pos += 1\n",
    "\n",
    "        recall = true_pos / tot_pos\n",
    "        recalls.append(recall)\n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 1\n",
    "        else:\n",
    "            precision = true_pos / (true_pos + false_pos)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        if precision == 0 and recall == 0:\n",
    "            fscore = 0\n",
    "        else:\n",
    "            fscore = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "        if fscore > best_fscore:\n",
    "            best_fscore = fscore\n",
    "            best_prec = precision\n",
    "            best_recall = recall\n",
    "\n",
    "        brain_ids.append(brain)\n",
    "    best_fscores[brain] = best_fscore\n",
    "    best_precisions.append(best_prec)\n",
    "    best_recalls.append(best_recall)\n",
    "\n",
    "\n",
    "for i, brain_id in enumerate(brain_ids):\n",
    "    brain_ids[i] = brain_id + f\" - Max F-score: {best_fscores[brain_id]:.2f}\"\n",
    "\n",
    "data = {\"Sample\": brain_ids, \"Recall\": recalls, \"Precision\": precisions}\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (10, 7)})\n",
    "sns.set(font_scale=2)\n",
    "sns.lineplot(data=df, x=\"Recall\", y=\"Precision\", hue=\"Sample\", estimator=\"max\", ci=None)\n",
    "sns.scatterplot(x=best_recalls, y=best_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine best threshld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(test_files, disable=True):\n",
    "    print(f\"*************File: {filename}*********\")\n",
    "    if filename in doubles:\n",
    "        newpos = 2\n",
    "    else:\n",
    "        newpos = 1\n",
    "\n",
    "    im_fname = files_dir + filename[:-17] + \".h5\"\n",
    "    fname = files_dir + filename\n",
    "    f = h5py.File(fname, \"r\")\n",
    "    pred = f.get(\"exported_data\")\n",
    "    pred = pred[0, :, :, :]\n",
    "    mask = pred > best_threshold\n",
    "    labels = measure.label(mask)\n",
    "    props = measure.regionprops(labels)\n",
    "\n",
    "    if \"pos\" in filename:\n",
    "        num_detected = 0\n",
    "        tot_pos += newpos\n",
    "        for prop in props:\n",
    "            area = prop[\"area\"]\n",
    "            if area > size_thresh:\n",
    "                print(f\"area of detected object: {area}\")\n",
    "                if num_detected < newpos:\n",
    "                    true_pos += 1\n",
    "                    num_detected += 1\n",
    "                else:\n",
    "                    print(f\"Soma false positive Area: {area}\")\n",
    "                    f = h5py.File(im_fname, \"r\")\n",
    "                    im = f.get(\"image_3channel\")\n",
    "                    viewer = napari.Viewer(ndisplay=3)\n",
    "                    viewer.add_image(im[0, :, :, :])\n",
    "                    viewer.add_image(im[1, :, :, :])\n",
    "                    viewer.add_image(im[2, :, :, :])\n",
    "                    viewer.add_labels(mask)\n",
    "                    viewer.add_labels(\n",
    "                        labels == prop[\"label\"],\n",
    "                        name=f\"soma false positive area: {area}\",\n",
    "                    )\n",
    "                    false_pos += 1\n",
    "        if num_detected == 0:\n",
    "            print(f\"Soma false negative\")\n",
    "            f = h5py.File(im_fname, \"r\")\n",
    "            im = f.get(\"image_3channel\")\n",
    "            viewer = napari.Viewer(ndisplay=3)\n",
    "            viewer.add_image(im[0, :, :, :])\n",
    "            viewer.add_image(im[1, :, :, :])\n",
    "            viewer.add_image(im[2, :, :, :])\n",
    "            viewer.add_labels(mask, name=\"Soma false negative\")\n",
    "    elif \"neg\" in filename:\n",
    "        for prop in props:\n",
    "            area = prop[\"area\"]\n",
    "            if area > size_thresh:\n",
    "                print(f\"Nonsoma false positive Area: {area}\")\n",
    "                f = h5py.File(im_fname, \"r\")\n",
    "                im = f.get(\"image_3channel\")\n",
    "                viewer = napari.Viewer(ndisplay=3)\n",
    "                viewer.add_image(im[0, :, :, :])\n",
    "                viewer.add_image(im[1, :, :, :])\n",
    "                viewer.add_image(im[2, :, :, :])\n",
    "                viewer.add_labels(mask)\n",
    "                viewer.add_labels(\n",
    "                    labels == prop[\"label\"], name=f\"nonsoma false positive area: {area}\"\n",
    "                )\n",
    "                false_pos += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"2867_4336_1296_pos_Probabilities.h5\"\n",
    "im_fname = files_dir + filename[:-17] + \".h5\"\n",
    "f = h5py.File(im_fname, \"r\")\n",
    "im = f.get(\"image_3channel\")\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(im[0, :, :, :])\n",
    "viewer.add_image(im[1, :, :, :])\n",
    "viewer.add_image(im[2, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make point layer - ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_path = \"precomputed://https://dlab-colm.neurodata.io/2021_10_06/8557/point_preds\"\n",
    "info = CloudVolume.create_new_info(\n",
    "    num_channels=1,\n",
    "    layer_type=\"segmentation\",\n",
    "    data_type=\"uint64\",  # Channel images might be 'uint8'\n",
    "    # raw, jpeg, compressed_segmentation, fpzip, kempressed, compresso\n",
    "    encoding=\"raw\",\n",
    "    resolution=[4, 4, 40],  # Voxel scaling, units are in nanometers\n",
    "    voxel_offset=[0, 0, 0],  # x,y,z offset in voxels from the origin\n",
    "    mesh=\"mesh\",\n",
    "    # Pick a convenient size for your underlying chunk representation\n",
    "    # Powers of two are recommended, doesn't need to cover image exactly\n",
    "    chunk_size=[512, 512, 16],  # units are voxels\n",
    "    volume_size=[250000, 250000, 25000],  # e.g. a cubic millimeter dataset\n",
    ")\n",
    "vol = CloudVolume(point_path, info=info)\n",
    "vol.commit_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_somas = []\n",
    "for soma in somas:\n",
    "    if soma[2] <= 3000:\n",
    "        new_somas.append(soma)\n",
    "len(new_somas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_ra = np.array(new_somas)\n",
    "plt.hist(soma_ra[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_fg = vol_fg[3100:3500, 4500:4900, 1400:1800, 0]\n",
    "im_bg = vol_bg[3100:3500, 4500:4900, 1400:1800, 0]\n",
    "im_endo = vol_endo[3100:3500, 4500:4900, 1400:1800, 0]\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(np.squeeze(im_fg))\n",
    "viewer.add_image(np.squeeze(im_bg))\n",
    "viewer.add_image(np.squeeze(im_endo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check ilastik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_fg = vol_fg[3100:3500, 4500:4900, 1400:1800, 0]\n",
    "im_bg = vol_bg[3100:3500, 4500:4900, 1400:1800, 0]\n",
    "im_endo = vol_endo[3100:3500, 4500:4900, 1400:1800, 0]\n",
    "\n",
    "image = np.squeeze(np.stack([im_fg, im_bg, im_endo], axis=0))\n",
    "fname = \"/Users/thomasathey/Desktop/im.h5\"\n",
    "\n",
    "with h5py.File(fname, \"w\") as f:\n",
    "    dset = f.create_dataset(\"image_3channel\", data=image)\n",
    "\n",
    "subprocess.run(\n",
    "    [\n",
    "        \"/Applications/ilastik-1.3.3post3-OSX.app/Contents/ilastik-release/run_ilastik.sh\",\n",
    "        \"--headless\",\n",
    "        \"--project=/Users/thomasathey/Documents/mimlab/mouselight/ailey/soma_detection/matt_soma_rabies_pix_3ch.ilp\",\n",
    "        fname,\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"/Users/thomasathey/Desktop/im_Probabilities.h5\", \"r\")\n",
    "pred = f.get(\"exported_data\")\n",
    "\n",
    "pred = pred[0, :, :, :]\n",
    "\n",
    "mask = pred > 0.55\n",
    "labels = measure.label(mask)\n",
    "props = measure.regionprops(labels)\n",
    "\n",
    "results = []\n",
    "for prop in props:\n",
    "    if prop[\"area\"] > 500:\n",
    "        location = list(np.add((0, 0, 0), prop[\"centroid\"]))\n",
    "        print(location)\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(np.squeeze(im_fg))\n",
    "viewer.add_labels(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "new_pts = []\n",
    "for point in a:\n",
    "    first = math.isclose(point[0], int(point[0]))\n",
    "    second = math.isclose(np.abs(point[1] - int(point[1])), 0.5)\n",
    "    third = math.isclose(np.abs(point[2] - int(point[2])), 0.5)\n",
    "    if first and second and third:\n",
    "        continue\n",
    "    else:\n",
    "        new_pts.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import napari\n",
    "\n",
    "subvol_number = -1  # change value here for different center point, valid values 0-9\n",
    "r = 400  # change for voxel radius\n",
    "\n",
    "vol = CloudVolume(\n",
    "    \"precomputed://https://dlab-colm.neurodata.io/2021_10_06/8557/Ch_647\",\n",
    "    parallel=1,\n",
    "    mip=0,\n",
    "    fill_missing=False,\n",
    ")\n",
    "print(f\"Total volume size: {vol.shape}\")\n",
    "\n",
    "\n",
    "centers = [\n",
    "    [1042.743408203125, 2647.38671875, 1224.5],\n",
    "    [2430.76171875, 3053.81494140625, 713.5],\n",
    "    [1780.8623046875, 2691.490234375, 1538.5001220703125],\n",
    "    [1642.8656005859375, 3693.246337890625, 1540.4998779296875],\n",
    "    [1924.112060546875, 2812.427978515625, 1539.5],\n",
    "    [2228.993896484375, 3124.892333984375, 710.4999389648438],\n",
    "    [976.0220336914062, 2615.152587890625, 1295.4998779296875],\n",
    "    [2973.654296875, 5300.09912109375, 900.5],\n",
    "    [3070.7158203125, 5557.70166015625, 1127.5000610351562],\n",
    "    [780.11474609375, 2957.698486328125, 1287.5],\n",
    "]\n",
    "\n",
    "center = centers[subvol_number]\n",
    "center = [int(c) for c in center]\n",
    "\n",
    "center = [536, 6588, 2800]\n",
    "\n",
    "lower = [c - r for c in center]\n",
    "upper = [c + r for c in center]\n",
    "print(f\"Downloading bbox: {lower} to {upper}\")\n",
    "\n",
    "\n",
    "im = vol[lower[0] : upper[0], lower[1] : upper[1], lower[2] : upper[2], 0]\n",
    "im = np.squeeze(im)\n",
    "\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "heights = np.array(\n",
    "    [70, 67, 73, 70, 66, 67, 68, 68, 74, 69, 71, 67, 70, 67, 67, 70, 67, 72, 71, 71]\n",
    ")\n",
    "bouldering = np.arange(1, len(heights) + 1)\n",
    "lead = np.array([11, 14, 4, 15, 2, 6, 1, 7, 9, 16, 5, 17, 12, 3, 8, 10, 13, 20, 19, 18])\n",
    "speed = [3, 2, 18, 16, 6, 19, 12, 20, 11, 5, 10, 15, 8, 7, 14, 9, 4, 1, 17, 13]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "m, b = np.polyfit(heights, bouldering, 1)\n",
    "coef, p = pearsonr(heights, bouldering)\n",
    "axs[0].scatter(heights, bouldering)\n",
    "axs[0].plot(heights, m * heights + b, label=f\"m={m:.2f}, coef={coef:.2f}, p={p:.2f}\")\n",
    "axs[0].set_title(\"Bouldering\")\n",
    "axs[0].set_ylabel(\"Finish in Olympic Qualifying Round\")\n",
    "axs[0].set_xlabel(\"Height (in.)\")\n",
    "axs[0].legend()\n",
    "\n",
    "m, b = np.polyfit(heights, lead, 1)\n",
    "coef, p = pearsonr(heights, lead)\n",
    "axs[1].scatter(heights, lead)\n",
    "axs[1].plot(heights, m * heights + b, label=f\"m={m:.2f}, coef={coef:.2f}, p={p:.2f}\")\n",
    "axs[1].set_title(\"Lead Climbing\")\n",
    "axs[1].legend()\n",
    "\n",
    "m, b = np.polyfit(heights, speed, 1)\n",
    "coef, p = pearsonr(heights, speed)\n",
    "axs[2].scatter(heights, speed)\n",
    "axs[2].plot(heights, m * heights + b, label=f\"m={m:.2f}, coef={coef:.2f}, p={p:.2f}\")\n",
    "axs[2].set_title(\"Speed Climbing\")\n",
    "axs[2].legend()\n",
    "fig.set_figwidth(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(heights, lead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axon mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = CloudVolume(\n",
    "    \"precomputed://https://open-neurodata.s3.amazonaws.com/brainlit/brain1_segments\",\n",
    "    mip=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.divide(vol.skeleton.get(2).vertices, vol.resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.skeleton.get(2).vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.voxel_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.skeleton.get(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dc00d68ff54f8375e99934614da4863299fb9e10af4294c095b7f517546ff26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('docs_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
