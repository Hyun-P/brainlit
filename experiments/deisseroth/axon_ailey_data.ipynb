{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "from skimage.transform import downscale_local_mean\n",
    "import napari\n",
    "from skimage import io\n",
    "import random\n",
    "import h5py\n",
    "from skimage import measure\n",
    "from brainlit.preprocessing import removeSmallCCs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import tables\n",
    "from napari_animation import AnimationWidget\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from parse_ara import *\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from statannotations.Annotator import Annotator\n",
    "import pandas as pd\n",
    "import brainrender\n",
    "import scipy.ndimage as ndi\n",
    "from skimage.morphology import skeletonize\n",
    "from axon_data import brain2paths, brain2centers\n",
    "import os\n",
    "from util import json_to_points\n",
    "\n",
    "%gui qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = \"8649\"\n",
    "\n",
    "base_dir = (\n",
    "    \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/brain\"\n",
    "    + brain\n",
    "    + \"/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if brain in brain2centers.keys():\n",
    "    centers_train = brain2centers[brain][0]\n",
    "    centers_val = brain2centers[brain][1]  # annotate z slice 25, 50 and 75\n",
    "    print(f\"{len(centers_train)} training samples, {len(centers_val)} val samples\")\n",
    "elif \"val_info\" in brain2paths[brain].keys():\n",
    "    centers_val = json_to_points(brain2paths[brain][\"val_info\"][\"url\"])[\n",
    "        brain2paths[brain][\"val_info\"][\"layer\"]\n",
    "    ]\n",
    "    print(f\"{len(centers_val)} val samples\")\n",
    "\n",
    "\n",
    "mip = 0\n",
    "\n",
    "dir = brain2paths[brain][\"ab\"]\n",
    "vol_fg = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "print(f\"fg shape: {vol_fg.shape} at {vol_fg.resolution}\")\n",
    "dir = brain2paths[brain][\"bg\"]\n",
    "vol_bg = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "print(f\"bg shape: {vol_bg.shape} at {vol_bg.resolution}\")\n",
    "dir = brain2paths[brain][\"endo\"]\n",
    "vol_endo = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "print(f\"endo shape: {vol_endo.shape} at {vol_endo.resolution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mask\" in brain2paths[brain].keys():\n",
    "    dir = brain2paths[brain][\"mask\"]\n",
    "    vol_mask = CloudVolume(dir, parallel=1, mip=mip, fill_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = brain2paths[brain][\"ab\"]\n",
    "vol_fg = CloudVolume(dir, parallel=1, mip=3, fill_missing=False)\n",
    "print(f\"fg shape: {vol_fg.shape} at {vol_fg.resolution}\")\n",
    "scale = [i / 1000 for i in vol_fg.resolution]\n",
    "scale[2] = scale[2] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.squeeze(np.array(vol_fg[:, :, :]))\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "new_array = zoom(im, (1, 1, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(new_array, scale=scale)\n",
    "viewer.scale_bar.visible = True\n",
    "viewer.scale_bar.unit = \"um\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ilastik - blue/1 is axno yellow/0 is bg\n",
    "# prediction model is /Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/brain3/matt_benchmark_formal_brain3.ilp\n",
    "dataset_to_save = \"val\"\n",
    "\n",
    "if dataset_to_save == \"train\":\n",
    "    centers = centers_train\n",
    "elif dataset_to_save == \"val\":\n",
    "    centers = centers_val\n",
    "else:\n",
    "    raise ValueError(\"invalid dataset\")\n",
    "\n",
    "for i, center in enumerate(centers):\n",
    "    print(center)\n",
    "    image_fg = vol_fg[\n",
    "        center[0] - 49 : center[0] + 50,\n",
    "        center[1] - 49 : center[1] + 50,\n",
    "        center[2] - 49 : center[2] + 50,\n",
    "    ]\n",
    "    image_fg = image_fg[:, :, :, 0]\n",
    "\n",
    "    image_bg = vol_bg[\n",
    "        center[0] - 49 : center[0] + 50,\n",
    "        center[1] - 49 : center[1] + 50,\n",
    "        center[2] - 49 : center[2] + 50,\n",
    "    ]\n",
    "    image_bg = image_bg[:, :, :, 0]\n",
    "\n",
    "    image_endo = vol_endo[\n",
    "        center[0] - 49 : center[0] + 50,\n",
    "        center[1] - 49 : center[1] + 50,\n",
    "        center[2] - 49 : center[2] + 50,\n",
    "    ]\n",
    "    image_endo = image_endo[:, :, :, 0]\n",
    "\n",
    "    image_2channel = np.stack([image_bg, image_fg, image_endo], axis=0)\n",
    "\n",
    "    fname = f\"{base_dir}{dataset_to_save}_{int(center[0])}_{int(center[1])}_{int(center[2])}.h5\"\n",
    "    with h5py.File(fname, \"w\") as f:\n",
    "        dset = f.create_dataset(\"image_2channel\", data=image_2channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = \"val\"\n",
    "num = \"3\"\n",
    "\n",
    "fname = base_dir + dset + \"_\" + num + \".h5\"\n",
    "\n",
    "with h5py.File(fname, \"r\") as f:\n",
    "    pred = f.get(\"image_2channel\")\n",
    "    image_bg = pred[0, :, :, :]\n",
    "    image_fg = pred[1, :, :, :]\n",
    "    image_endo = pred[2, :, :, :]\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(image_fg)\n",
    "viewer.add_image(image_bg)\n",
    "viewer.add_image(image_endo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(base_dir)\n",
    "files = [base_dir + f for f in files if \"val\" in f]\n",
    "files = [f for f in files if \"Labels.h5\" in f]\n",
    "\n",
    "num_pos_voxels = 0\n",
    "for fname in files:\n",
    "    f = h5py.File(fname, \"r\")\n",
    "    gt = f.get(\"exported_data\")\n",
    "    gt = gt[0, :, :, :]\n",
    "    pos_labels = gt == 2\n",
    "    num_pos_voxels += np.sum(pos_labels)\n",
    "\n",
    "print(num_pos_voxels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brains = [\"8613\", \"3\", \"4\"]\n",
    "\n",
    "for brain in brains:\n",
    "    base_dir = (\n",
    "        \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/benchmark_formal/brain\"\n",
    "        + brain\n",
    "        + \"/\"\n",
    "    )\n",
    "\n",
    "    files = os.listdir(base_dir)\n",
    "    files = [base_dir + f for f in files if \"train\" in f]\n",
    "    files = [f for f in files if \"Labels\" in f]\n",
    "    # need to filter for labels/probs fiiles\n",
    "\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    for fname in files:\n",
    "        f = h5py.File(fname, \"r\")\n",
    "        im = np.array(f.get(\"exported_data\"))\n",
    "        pos = np.sum(im == 2)\n",
    "        neg = np.sum(im == 1)\n",
    "\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "\n",
    "    print(f\"{brain}: {total_pos}+/{total_neg}- total voxels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Val results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 0.02\n",
    "thresholds = np.arange(spacing, 1.0, spacing)\n",
    "precisions = []\n",
    "recalls = []\n",
    "best_fscore = 0\n",
    "\n",
    "files = os.listdir(base_dir)\n",
    "files = [base_dir + f for f in files if \"val\" in f]\n",
    "files = [f for f in files if \"_Probabilities.h5\" in f]\n",
    "print(f\"{len(files)} total validation subvolumes\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    true_pos_total = 0\n",
    "    false_pos_total = 0\n",
    "    true_labels_total = 0\n",
    "    true_labels_total_neg = 0\n",
    "    for fname_prob in files:\n",
    "\n",
    "        fname_im = fname_prob[:-17] + \".h5\"\n",
    "        f = h5py.File(fname_im, \"r\")\n",
    "        im = f.get(\"image_2channel\")\n",
    "        im_bg = im[0, :, :, :]\n",
    "        im_fg = im[1, :, :, :]\n",
    "\n",
    "        fname_lab = fname_prob[:-17] + \"-image_2channel_Labels.h5\"\n",
    "        f = h5py.File(fname_lab, \"r\")\n",
    "        gt = f.get(\"exported_data\")\n",
    "        gt = gt[0, :, :, :]\n",
    "        pos_labels = gt == 2\n",
    "        neg_labels = gt == 1\n",
    "\n",
    "        f = h5py.File(fname_prob, \"r\")\n",
    "        seg = f.get(\"exported_data\")\n",
    "        seg = seg[1, :, :, :]\n",
    "        mask = seg > threshold\n",
    "\n",
    "        true_pos = np.sum(np.logical_and(mask, pos_labels))\n",
    "        true_pos_total += true_pos\n",
    "        false_pos = np.sum(np.logical_and(mask, gt == 1))\n",
    "        false_pos_total += false_pos\n",
    "        true_labels = np.sum(pos_labels)\n",
    "        true_labels_total += true_labels\n",
    "        true_labels_neg = np.sum(neg_labels)\n",
    "        true_labels_total_neg += true_labels_neg\n",
    "\n",
    "    precision_total = true_pos_total / (true_pos_total + false_pos_total)\n",
    "    recall_total = true_pos_total / true_labels_total\n",
    "    fscore = 2 / (1 / precision_total + 1 / recall_total)\n",
    "    print(\n",
    "        f\"Thresh: {threshold:.2f} --- Total prec.: {precision_total:.3f} total rec.: {recall_total:.3f} w/{true_labels_total}/{true_labels_total_neg} total pos/neg voxels. F-score: {fscore:.4f}\"\n",
    "    )\n",
    "    if fscore > best_fscore:\n",
    "        best_fscore = fscore\n",
    "        best_prec = precision_total\n",
    "        best_recall = recall_total\n",
    "        best_threshold = threshold\n",
    "    precisions.append(precision_total)\n",
    "    recalls.append(recall_total)\n",
    "plt.plot(recalls, precisions, label=\"Prec-Rec Curve\")\n",
    "plt.scatter(\n",
    "    [best_recall],\n",
    "    [best_prec],\n",
    "    c=\"red\",\n",
    "    label=f\"Best F-score: {best_fscore:.3f} (thresh {best_threshold:.2f})\",\n",
    ")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.title(f\"Validation Brain {brain} w/{true_labels_total} Total Pos. Voxels\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(base_dir)\n",
    "files = [base_dir + f for f in files if \"val\" in f]\n",
    "files = [f for f in files if \"_Probabilities.h5\" in f]\n",
    "print(f\"{len(files)} total validation subvolumes\")\n",
    "\n",
    "\n",
    "for i, fname_prob in enumerate(files):\n",
    "    fname_im = fname_prob[:-17] + \".h5\"\n",
    "    f = h5py.File(fname_im, \"r\")\n",
    "    im = f.get(\"image_2channel\")\n",
    "    im_bg = im[0, :, :, :]\n",
    "    im_fg = im[1, :, :, :]\n",
    "\n",
    "    fname_lab = fname_prob[:-17] + \"-image_2channel_Labels.h5\"\n",
    "    f = h5py.File(fname_lab, \"r\")\n",
    "    gt = f.get(\"exported_data\")\n",
    "    gt = gt[0, :, :, :]\n",
    "    pos_labels = gt == 2\n",
    "    neg_labels = gt == 1\n",
    "\n",
    "    f = h5py.File(fname_prob, \"r\")\n",
    "    seg = f.get(\"exported_data\")\n",
    "    seg = seg[1, :, :, :]\n",
    "    mask = seg > best_threshold\n",
    "\n",
    "    true_pos = np.sum(np.logical_and(mask, pos_labels))\n",
    "    false_pos = np.sum(np.logical_and(mask, gt == 1))\n",
    "    true_labels = np.sum(pos_labels)\n",
    "    true_labels_neg = np.sum(neg_labels)\n",
    "\n",
    "    if true_labels == 0:\n",
    "        recall = 1\n",
    "    else:\n",
    "        recall = true_pos/true_labels\n",
    "\n",
    "    if true_pos + false_pos == 0:\n",
    "        precision = 1\n",
    "    else:\n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "\n",
    "    if precision < 0.8 or recall < 0.8:\n",
    "        print(f\"{i}) {fname_prob}: prec{precision} recall: {recall}\")\n",
    "        viewer = napari.Viewer(ndisplay=3)\n",
    "        viewer.add_image(im_fg, name=f\"fg {i}\")\n",
    "        viewer.add_image(im_bg, name=\"bg\")\n",
    "        viewer.add_labels(mask, name=\"mask\")\n",
    "        viewer.add_labels(pos_labels+2*neg_labels, name=\"pos labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper figure for all validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brains = [\"8613\", \"3\", \"4\", \"8604\", \"8650\", \"8589\", \"8590\"]\n",
    "\n",
    "brain_ids = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_fscores = {}\n",
    "\n",
    "for brain in brains:\n",
    "\n",
    "    base_dir = (\n",
    "        \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/brain\"\n",
    "        + brain\n",
    "        + \"/\"\n",
    "    )\n",
    "\n",
    "    spacing = 0.02\n",
    "    thresholds = np.arange(spacing, 1.0, spacing)\n",
    "    best_fscore = 0\n",
    "\n",
    "    files = os.listdir(base_dir)\n",
    "    files = [base_dir + f for f in files if \"val\" in f]\n",
    "    files = [f for f in files if \"_Probabilities.h5\" in f]\n",
    "    fiiles = [f for f in files if \"val\" in f]\n",
    "\n",
    "    print(f\"{len(files)} total validation subvolumes for brain {brain}\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        true_pos_total = 0\n",
    "        false_pos_total = 0\n",
    "        true_labels_total = 0\n",
    "        true_labels_total_neg = 0\n",
    "        for fname_prob in files:\n",
    "\n",
    "            fname_im = fname_prob[:-17] + \".h5\"\n",
    "            f = h5py.File(fname_im, \"r\")\n",
    "            im = f.get(\"image_2channel\")\n",
    "            im_bg = im[0, :, :, :]\n",
    "            im_fg = im[1, :, :, :]\n",
    "\n",
    "            fname_lab = fname_prob[:-17] + \"-image_2channel_Labels.h5\"\n",
    "            f = h5py.File(fname_lab, \"r\")\n",
    "            gt = f.get(\"exported_data\")\n",
    "            gt = gt[0, :, :, :]\n",
    "            pos_labels = gt == 2\n",
    "            neg_labels = gt == 1\n",
    "\n",
    "            f = h5py.File(fname_prob, \"r\")\n",
    "            seg = f.get(\"exported_data\")\n",
    "            seg = seg[1, :, :, :]\n",
    "            mask = seg > threshold\n",
    "\n",
    "            true_pos = np.sum(np.logical_and(mask, pos_labels))\n",
    "            true_pos_total += true_pos\n",
    "            false_pos = np.sum(np.logical_and(mask, gt == 1))\n",
    "            false_pos_total += false_pos\n",
    "            true_labels = np.sum(pos_labels)\n",
    "            true_labels_total += true_labels\n",
    "            true_labels_neg = np.sum(neg_labels)\n",
    "            true_labels_total_neg += true_labels_neg\n",
    "\n",
    "        precision_total = true_pos_total / (true_pos_total + false_pos_total)\n",
    "        recall_total = true_pos_total / true_labels_total\n",
    "\n",
    "        precisions.append(precision_total)\n",
    "        recalls.append(recall_total)\n",
    "        brain_ids.append(brain)\n",
    "\n",
    "        fscore = 2 / (1 / precision_total + 1 / recall_total)\n",
    "\n",
    "        if fscore > best_fscore:\n",
    "            best_fscore = fscore\n",
    "            best_prec = precision_total\n",
    "            best_recall = recall_total\n",
    "            best_threshold = threshold\n",
    "    best_precisions.append(best_prec)\n",
    "    best_recalls.append(best_recall)\n",
    "    best_fscores[brain] = best_fscore\n",
    "for i, brain_id in enumerate(brain_ids):\n",
    "    brain_ids[i] = brain_id + f\" - Max F-score: {best_fscores[brain_id]:.2f}\"\n",
    "\n",
    "data = {\"Sample\": brain_ids, \"Recall\": recalls, \"Precision\": precisions}\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (10, 7)})\n",
    "sns.set(font_scale=2)\n",
    "sns.lineplot(data=df, x=\"Recall\", y=\"Precision\", hue=\"Sample\")\n",
    "sns.scatterplot(x=best_recalls, y=best_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=best_recalls, y=best_precisions, hue=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_points(\n",
    "    \"https://viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=Tizkn7VMAv6B6Q\",\n",
    "    round=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname_prob in files:\n",
    "    fname_im = fname_prob[:-17] + \".h5\"\n",
    "    f = h5py.File(fname_im, \"r\")\n",
    "    im = f.get(\"image_2channel\")\n",
    "    im_bg = im[0, :, :, :]\n",
    "    im_fg = im[1, :, :, :]\n",
    "    im_endo = im[2, :, :, :]\n",
    "\n",
    "    fname_lab = fname_prob[:-17] + \"-image_2channel_Labels.h5\"\n",
    "    f = h5py.File(fname_lab, \"r\")\n",
    "    gt = f.get(\"exported_data\")\n",
    "    gt = gt[0, :, :, :]\n",
    "    pos_labels = gt == 2\n",
    "\n",
    "    f = h5py.File(fname_prob, \"r\")\n",
    "    seg = f.get(\"exported_data\")\n",
    "    seg = seg[1, :, :, :]\n",
    "    mask = seg > best_threshold\n",
    "\n",
    "    true_pos = np.sum(np.logical_and(mask, pos_labels))\n",
    "    true_pos_total += true_pos\n",
    "    false_pos = np.sum(np.logical_and(mask, gt == 1))\n",
    "    false_pos_total += false_pos\n",
    "    true_labels = np.sum(pos_labels)\n",
    "    true_labels_total += true_labels\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / true_labels\n",
    "    fscore = 2 / (1 / precision + 1 / recall)\n",
    "    print(f\"prec {precision} rec {recall} f {fscore}\")\n",
    "    if fscore < 0.75:\n",
    "        print(fname_prob)\n",
    "        name = fname_prob.split(\"/\")[-1]\n",
    "        viewer = napari.Viewer(ndisplay=3)\n",
    "        viewer.add_image(im_bg, name=name[:-17])\n",
    "        viewer.add_image(im_fg)\n",
    "        viewer.add_image(im_endo)\n",
    "        viewer.add_labels(gt)\n",
    "        viewer.add_labels(mask, name=f\"p:{precision:.2f} r{recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_points(\n",
    "    \"https://viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=V1ZmxgI5NeFOaw\",\n",
    "    round=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make annotation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = brain2paths[brain][\"mask\"]\n",
    "\n",
    "\n",
    "info = CloudVolume.create_new_info(\n",
    "    num_channels=1,\n",
    "    layer_type=\"segmentation\",\n",
    "    data_type=\"uint64\",  # Channel images might be 'uint8'\n",
    "    encoding=\"raw\",  # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "    resolution=vol_bg.resolution,  # Voxel scaling, units are in nanometers\n",
    "    voxel_offset=vol_bg.voxel_offset,  # x,y,z offset in voxels from the origin\n",
    "    # mesh            = 'mesh',\n",
    "    # Pick a convenient size for your underlying chunk representation\n",
    "    # Powers of two are recommended, doesn't need to cover image exactly\n",
    "    chunk_size=[128, 128, 2],  # units are voxels\n",
    "    volume_size=vol_bg.volume_size,  # e.g. a cubic millimeter dataset\n",
    ")\n",
    "vol_mask = CloudVolume(dir, info=info)\n",
    "vol_mask.commit_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cannot write to https link, can write to s3 link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check whole brain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from cloud\n",
    "centers = [[2304, 3840, 2100], [2560, 3840, 1800]]\n",
    "\n",
    "for i, center in enumerate(centers):\n",
    "    image_fg = vol_fg[\n",
    "        center[0] : center[0] + 256,\n",
    "        center[1] : center[1] + 256,\n",
    "        center[2] : center[2] + 300,\n",
    "    ]\n",
    "    image_fg = image_fg[:, :, :, 0]\n",
    "\n",
    "    image_bg = vol_bg[\n",
    "        center[0] : center[0] + 256,\n",
    "        center[1] : center[1] + 256,\n",
    "        center[2] : center[2] + 300,\n",
    "    ]\n",
    "    image_bg = image_bg[:, :, :, 0]\n",
    "\n",
    "    mask_s3 = vol_mask[\n",
    "        center[0] : center[0] + 256,\n",
    "        center[1] : center[1] + 256,\n",
    "        center[2] : center[2] + 300,\n",
    "    ]\n",
    "    mask_s3 = mask_s3[:, :, :, 0]\n",
    "\n",
    "    image_2channel = np.stack([image_bg, image_fg], axis=0)\n",
    "    fname = (\n",
    "        \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/benchmark_formal/test_\"\n",
    "        + str(i)\n",
    "        + \".h5\"\n",
    "    )\n",
    "    with h5py.File(fname, \"w\") as f:\n",
    "        dset = f.create_dataset(\"image_2channel\", data=image_2channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/image_0.h5\"\n",
    "\n",
    "subprocess.run(\n",
    "    [\n",
    "        \"/Applications/ilastik-1.3.3post3-OSX.app/Contents/ilastik-release/run_ilastik.sh\",\n",
    "        \"--headless\",\n",
    "        \"--project=/Users/thomasathey/Documents/mimlab/mouselight/ailey/benchmark_formal/matt_benchmark_formal.ilp\",\n",
    "        fname,\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample axon mask layer with igneous (this is only partial code, see igneous github for other code snippets)\n",
    "\n",
    "layer_path = \"s3://smartspim-precomputed-volumes/2021_07_15_Sert_Cre_R/axon_mask\"\n",
    "\n",
    "tasks = tc.create_downsampling_tasks(\n",
    "    layer_path,  # e.g. 'gs://bucket/dataset/layer'\n",
    "    mip=0,  # Start downsampling from this mip level (writes to next level up)\n",
    "    fill_missing=True,  # Ignore missing chunks and fill them with black\n",
    "    axis=\"z\",\n",
    "    num_mips=5,  # number of downsamples to produce. Downloaded shape is chunk_size * 2^num_mip\n",
    "    chunk_size=None,  # manually set chunk size of next scales, overrides preserve_chunk_size\n",
    "    preserve_chunk_size=True,  # use existing chunk size, don't halve to get more downsamples\n",
    "    sparse=False,  # for sparse segmentation, allow inflation of pixels against background\n",
    "    bounds=None,  # mip 0 bounding box to downsample\n",
    "    encoding=None,  # e.g. 'raw', 'compressed_segmentation', etc\n",
    "    delete_black_uploads=False,  # issue a delete instead of uploading files containing all background\n",
    "    background_color=0,  # Designates the background color\n",
    "    compress=\"gzip\",  # None, 'gzip', and 'br' (brotli) are options\n",
    "    factor=(2, 2, 2),  # common options are (2,2,1) and (2,2,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download higher mip\n",
    "\n",
    "dir = \"precomputed://https://dlab-colm.neurodata.io/2021_07_15_Sert_Cre_R/axon_mask\"\n",
    "vol_mask_ds = CloudVolume(dir, parallel=1, mip=1, fill_missing=False)\n",
    "print(vol_mask_ds.shape)\n",
    "\n",
    "data = vol_mask_ds[:, :, 0, 0]\n",
    "data = data.astype(\"int8\")\n",
    "print(data.nbytes)\n",
    "# print(np.unique(data))\n",
    "data = data[:, :, :, 0]\n",
    "data = np.swapaxes(data, 0, 2)  # must do this\n",
    "print(data.shape)\n",
    "\n",
    "io.imsave(\n",
    "    \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/brain4/register/axon_mask_.tif\",\n",
    "    data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View coronal heat maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = brain2paths[brain][\"mask\"]\n",
    "vol_mask = CloudVolume(dir, parallel=1, mip=3, fill_missing=True)\n",
    "print(vol_mask.shape)\n",
    "\n",
    "\n",
    "dir = brain2paths[brain][\"atlas\"]\n",
    "vol_atlas = CloudVolume(dir, parallel=1, mip=0, fill_missing=True)\n",
    "print(vol_atlas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z_atlas in range(0, vol_atlas.shape[2], 1000):\n",
    "    atlas = np.squeeze(vol_atlas[:, :, z_atlas, 0])\n",
    "    mask = np.squeeze(vol_mask[:, :, int(z_atlas / 8), 0])\n",
    "    viewer = napari.Viewer(ndisplay=2)\n",
    "    viewer.add_image(mask)\n",
    "    viewer.add_labels(atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "im_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/brain4/register/registered_2.img\"\n",
    "im = io.imread(im_path)\n",
    "print(im.shape)\n",
    "print(np.unique(im))\n",
    "\n",
    "im = im.astype(\"float\")\n",
    "\n",
    "im_ds = ndi.zoom(im, (0.4, 0.4, 0.4))\n",
    "print(im_ds.shape)\n",
    "print(np.unique(im_ds))\n",
    "\n",
    "np.save(\n",
    "    \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/brain4/register/registered_3_ds.npy\",\n",
    "    im_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/brain4/register/axon_mask_1_0.tif\"\n",
    "im = io.imread(im_path)\n",
    "print(im.shape)\n",
    "print(np.sum(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = \"brain3\"\n",
    "\n",
    "# im_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/brain3/register/registered_3_ds.npy\"\n",
    "# im = np.load(im_path)\n",
    "# print(im.shape)\n",
    "\n",
    "# im_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/brain3/register/axon_mask_3.tif\"\n",
    "# im_unreg = io.imread(im_path)\n",
    "# print(im_unreg.shape)\n",
    "\n",
    "\n",
    "im_path = (\n",
    "    \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/\"\n",
    "    + brain\n",
    "    + \"/register/registered_1.img\"\n",
    ")\n",
    "im_reg2 = io.imread(im_path)\n",
    "print(im_reg2.shape)\n",
    "\n",
    "# im_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/\" + brain + \"/register/registered_3.img\"\n",
    "# im_reg3 = io.imread(im_path)\n",
    "# print(im_reg3.shape)\n",
    "\n",
    "# vol = CloudVolume(\n",
    "#     \"s3://open-neurodata/ara_2016/sagittal_10um/annotation_10um_2017\", mip=0, use_https=True\n",
    "# )\n",
    "# print(vol.shape)\n",
    "# atlas = vol[:,:,:,:]\n",
    "# atlas = np.squeeze(atlas).T\n",
    "# print(atlas.shape)\n",
    "# len(np.unique(atlas))\n",
    "\n",
    "im_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/ara_10um.tif\"\n",
    "atlas = io.imread(im_path)\n",
    "print(atlas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = ndi.gaussian_filter(im_reg2.astype(\"float\"), sigma=3)\n",
    "# smooth = ndi.zoom(smooth, (0.4,0.4,0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer(ndisplay=3)\n",
    "animation_widget = AnimationWidget(viewer)\n",
    "viewer.window.add_dock_widget(animation_widget, area=\"right\")\n",
    "# viewer.add_image(im)\n",
    "viewer.add_image(smooth)\n",
    "# viewer.add_image(im_reg2)\n",
    "# viewer.add_image(im_reg3)\n",
    "viewer.add_labels(atlas)\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in [180, 430, 680, 1030, 1280]:\n",
    "\n",
    "    slice = atlas[:, :, z]\n",
    "    slice_data = smooth[:, :, z]  # np.sum(im_reg[:,:,z], axis=2)\n",
    "    mn = np.amin(slice_data)\n",
    "    print(np.unique(slice_data))\n",
    "    slice_data[slice == 0] = mn\n",
    "    labels = measure.label(slice)\n",
    "    # plt.imshow(labels)\n",
    "\n",
    "    borders = 0 * labels\n",
    "    for label in np.unique(labels):\n",
    "        if label != 0:\n",
    "            mask = np.array(labels == label, dtype=\"int\")\n",
    "            erode = np.array(ndi.binary_erosion(mask))\n",
    "            outline = mask - erode\n",
    "            borders += outline\n",
    "\n",
    "    print(np.unique(borders))\n",
    "    borders = borders.astype(\"float\")\n",
    "    borders_layer = np.zeros((borders.shape[0], borders.shape[1], 4))\n",
    "    for rgba in range(borders_layer.shape[2]):\n",
    "        borders_layer[:, :, rgba] = borders\n",
    "\n",
    "    slice_data = ndi.rotate(slice_data, 270)\n",
    "    plt.imshow(slice_data, cmap=\"inferno\")\n",
    "    borders_layer = ndi.rotate(borders_layer, 270)\n",
    "    plt.imshow(borders_layer, cmap=\"gray\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    fig.savefig(\"/Users/thomasathey/Desktop/\" + str(z) + \".png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atlas readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = {}\n",
    "for x in tqdm(np.arange(0, vol_mask.shape[0], 128)):\n",
    "    x2 = np.amin([x + 128, vol_mask.shape[0]])\n",
    "    for y in tqdm(np.arange(0, vol_mask.shape[1], 128), leave=False):\n",
    "        y2 = np.amin([x + 128, vol_mask.shape[1]])\n",
    "        for z in tqdm(np.arange(0, vol_mask.shape[2], 128), leave=False):\n",
    "            z2 = np.amin([x + 128, vol_mask.shape[2]])\n",
    "            labels = vol_reg[x:x2, y:y2, z:z2]\n",
    "            labels_unique = np.unique(labels)\n",
    "            mask = vol_mask[x:x2, y:y2, z:z2]\n",
    "\n",
    "            for unq in labels_unique:\n",
    "                if unq in volumes.keys():\n",
    "                    cur_vol = volumes[unq][1]\n",
    "                    cur_total = volumes[unq][0]\n",
    "                else:\n",
    "                    cur_vol = 0\n",
    "                    cur_total = 0\n",
    "                cur_vol += np.sum(mask[labels == unq])\n",
    "                cur_total += np.sum(labels == unq)\n",
    "                volumes[unq] = [cur_total, cur_vol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read quantification dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brains = {\n",
    "    # \"3\": \"sert cre\",\n",
    "    # \"4\": \"sert cre\",\n",
    "    \"8613\": \"tph2 vglut3\",\n",
    "    # \"8604\": \"tbd\",\n",
    "    \"8650\": \"tph2 gad2\",\n",
    "    # \"8589\": \"tph2 vglut3\",\n",
    "    \"8590\": \"tph2 vglut3\",\n",
    "\n",
    "}\n",
    "counts = {}\n",
    "for gene in set(brains.values()):\n",
    "    count = 0\n",
    "    for brain in brains.keys():\n",
    "        if brains[brain] == gene:\n",
    "            count += 1\n",
    "    counts[gene] = count\n",
    "\n",
    "\n",
    "quantification_dicts = {}\n",
    "\n",
    "for brain in brains.keys():\n",
    "    path = (\n",
    "        \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/wholebrain_results/wholebrain_\"\n",
    "        + brain\n",
    "        + \".pkl\"\n",
    "    )\n",
    "    with open(path, \"rb\") as f:\n",
    "        quantification_dict = pickle.load(f)\n",
    "\n",
    "    quantification_dicts[brain] = quantification_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [688, 698, 1089, 583, 477, 803, 703, 1097, 549, 313, 1065]\n",
    "allen_regions = [\n",
    "    315,\n",
    "    698,\n",
    "    1089,\n",
    "    703,\n",
    "    477,\n",
    "    803,\n",
    "    549,\n",
    "    1097,\n",
    "    313,\n",
    "    771,\n",
    "    354,\n",
    "    512,\n",
    "]  # https://connectivity.brain-map.org/projection/experiment/480074702?imageId=480075280&initImage=TWO_PHOTON&x=17028&y=11704&z=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = json.load(\n",
    "    open(\n",
    "        \"/Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainlit/experiments/deisseroth/ara_structure_ontology.json\",\n",
    "        \"r\",\n",
    "    )\n",
    ")\n",
    "\n",
    "tree = build_tree(f)\n",
    "stack = [tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [tree]\n",
    "cur_level = -1\n",
    "counter = 0\n",
    "G = nx.DiGraph()\n",
    "max_level = 0\n",
    "\n",
    "\n",
    "while len(queue) > 0:\n",
    "    node = queue.pop(0)\n",
    "    if node.level > max_level:\n",
    "        max_level = node.level\n",
    "    G.add_node(\n",
    "        node.id,\n",
    "        level=node.level,\n",
    "        st_level=node.st_level,\n",
    "        name=node.name,\n",
    "        acronym=node.acronym,\n",
    "        label=str(node.st_level) + \") \" + node.name,\n",
    "    )\n",
    "    for brain in quantification_dicts.keys():\n",
    "        G.nodes[node.id][brain + \" axon\"] = 0\n",
    "        G.nodes[node.id][brain + \" total\"] = 0\n",
    "    if node.parent_id is not None:\n",
    "        G.add_edge(node.parent_id, node.id)\n",
    "\n",
    "    queue += node.children\n",
    "\n",
    "i_test = 0\n",
    "print(f\"Max level: {max_level}\")\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" axon\"])\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" total\"])\n",
    "for brain, quantification_dict in quantification_dicts.items():\n",
    "    for key in quantification_dict.keys():\n",
    "        if key in G.nodes:\n",
    "            G.nodes[key][brain + \" axon\"] = G.nodes[key][brain + \" axon\"] + float(\n",
    "                quantification_dict[key][1]\n",
    "            )\n",
    "            G.nodes[key][brain + \" total\"] = G.nodes[key][brain + \" total\"] + float(\n",
    "                quantification_dict[key][0]\n",
    "            )\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" axon\"])\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" total\"])\n",
    "\n",
    "for brain in quantification_dicts.keys():\n",
    "    for lvl in range(max_level, 0, -1):\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node][\"level\"] == lvl:\n",
    "                parent = list(G.in_edges(node))[0][0]\n",
    "                G.nodes[parent][brain + \" axon\"] = (\n",
    "                    G.nodes[parent][brain + \" axon\"] + G.nodes[node][brain + \" axon\"]\n",
    "                )\n",
    "                G.nodes[parent][brain + \" total\"] = (\n",
    "                    G.nodes[parent][brain + \" total\"] + G.nodes[node][brain + \" total\"]\n",
    "                )\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" axon\"])\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas + seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = {}\n",
    "\n",
    "for brain in quantification_dicts.keys():\n",
    "    total = 0\n",
    "    for node in G.nodes:\n",
    "        total += G.nodes[node][brain + \" axon\"]\n",
    "    totals[brain] = total\n",
    "\n",
    "axon_vols = []\n",
    "axon_denss = []\n",
    "gene = []\n",
    "subregion_name = []\n",
    "region_name = []\n",
    "for region in regions:\n",
    "    print(f\"Populating: \" + G.nodes[region][\"name\"])\n",
    "    #choose level here\n",
    "    children = list(G.successors(region))\n",
    "    #children = [region]\n",
    "    for child in children:\n",
    "        for brain in quantification_dicts.keys():\n",
    "            axon_vols.append(G.nodes[child][brain + \" axon\"] / totals[brain] * 100)\n",
    "            if (\n",
    "                G.nodes[child][brain + \" total\"] == 0\n",
    "                and G.nodes[child][brain + \" axon\"] == 0\n",
    "            ):\n",
    "                axon_denss.append(0)\n",
    "            elif G.nodes[child][brain + \" total\"] == 0:\n",
    "                raise ValueError(\"positive axon volume in zero volume region?\")\n",
    "            else:\n",
    "                axon_denss.append(\n",
    "                    G.nodes[child][brain + \" axon\"]\n",
    "                    / G.nodes[child][brain + \" total\"]\n",
    "                    * 100\n",
    "                )\n",
    "\n",
    "            gene.append(brains[brain] + f\" (n={counts[brains[brain]]})\")\n",
    "            subregion_name.append(G.nodes[child][\"name\"])\n",
    "            region_name.append(G.nodes[region][\"name\"])\n",
    "\n",
    "d = {\n",
    "    \"Percent Total Axon Volume (%)\": axon_vols,\n",
    "    \"Axon Density (%)\": axon_denss,\n",
    "    \"Gene\": gene,\n",
    "    \"Subregion\": subregion_name,\n",
    "    \"Region\": region_name,\n",
    "}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(30, 20))\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "#density\n",
    "fig_args = {\n",
    "    \"y\": \"Axon Density (%)\",\n",
    "    \"x\": \"Subregion\",\n",
    "    \"hue\": \"Gene\",\n",
    "    \"data\": df,\n",
    "}\n",
    "pairs = []\n",
    "unq_subregions = []\n",
    "for subregion in subregion_name:\n",
    "    if subregion not in unq_subregions:\n",
    "        unq_subregions.append(subregion)\n",
    "\n",
    "\n",
    "genes = df[\"Gene\"].unique()\n",
    "gene_pairs = [(a,b) for idx, a in enumerate(genes) for b in genes[idx+1:]]\n",
    "\n",
    "for gene_pair in gene_pairs:\n",
    "    for subregion in unq_subregions:\n",
    "        pairs.append(\n",
    "            (\n",
    "                (subregion, gene_pair[0]),\n",
    "                (subregion, gene_pair[1]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "annotator = Annotator(axes[0], pairs, **fig_args)\n",
    "fig_args = {\n",
    "    \"x\": \"Axon Density (%)\",\n",
    "    \"y\": \"Subregion\",\n",
    "    \"hue\": \"Gene\",\n",
    "    \"data\": df,\n",
    "}\n",
    "\n",
    "strpplot = sns.barplot(ax=axes[0], orient=\"h\", **fig_args)\n",
    "\n",
    "annotator.configure(test=\"Mann-Whitney\", text_format=\"star\", loc=\"outside\")\n",
    "annotator.new_plot(strpplot, orient=\"h\", plot=\"barplot\", **fig_args)\n",
    "annotator.apply_and_annotate()\n",
    "\n",
    "# percent total\n",
    "fig_args = {\n",
    "    \"y\": \"Percent Total Axon Volume (%)\",\n",
    "    \"x\": \"Subregion\",\n",
    "    \"hue\": \"Gene\",\n",
    "    \"data\": df,\n",
    "}\n",
    "pairs = []\n",
    "unq_subregions = []\n",
    "for subregion in subregion_name:\n",
    "    if subregion not in unq_subregions:\n",
    "        unq_subregions.append(subregion)\n",
    "\n",
    "\n",
    "for gene_pair in gene_pairs:\n",
    "    for subregion in unq_subregions:\n",
    "        pairs.append(\n",
    "            (\n",
    "                (subregion, gene_pair[0]),\n",
    "                (subregion, gene_pair[1]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "annotator = Annotator(axes[1], pairs, **fig_args)\n",
    "fig_args = {\n",
    "    \"x\": \"Percent Total Axon Volume (%)\",\n",
    "    \"y\": \"Subregion\",\n",
    "    \"hue\": \"Gene\",\n",
    "    \"data\": df,\n",
    "}\n",
    "\n",
    "strpplot = sns.barplot(ax=axes[1], orient=\"h\", **fig_args)\n",
    "strpplot.set_xscale(\"log\")\n",
    "\n",
    "annotator.configure(test=\"Mann-Whitney\", text_format=\"star\", loc=\"outside\")\n",
    "annotator.new_plot(strpplot, orient=\"h\", plot=\"barplot\", **fig_args)\n",
    "annotator.apply_and_annotate()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Allen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_denss = []\n",
    "gene = []\n",
    "subregion_name = []\n",
    "region_name = []\n",
    "subregions_list = []\n",
    "for region in allen_regions:\n",
    "    print(f\"Populating: \" + G.nodes[region][\"name\"])\n",
    "    children = list(G.successors(region))\n",
    "    for child in children:\n",
    "        if child not in subregions_list:\n",
    "            subregions_list.append(child)\n",
    "\n",
    "        for brain in quantification_dicts.keys():\n",
    "            if (\n",
    "                G.nodes[child][brain + \" total\"] == 0\n",
    "                and G.nodes[child][brain + \" axon\"] == 0\n",
    "            ):\n",
    "                axon_denss.append(0)\n",
    "            elif G.nodes[child][brain + \" total\"] == 0:\n",
    "                raise ValueError(\"positive axon volume in zero volume region?\")\n",
    "            else:\n",
    "                axon_denss.append(\n",
    "                    G.nodes[child][brain + \" axon\"] / G.nodes[child][brain + \" total\"]\n",
    "                )\n",
    "\n",
    "            if brain in [\"B\", \"R\"]:\n",
    "                gene.append(brain)\n",
    "            subregion_name.append(G.nodes[child][\"name\"])\n",
    "\n",
    "    region_name.append(G.nodes[region][\"name\"])\n",
    "\n",
    "tree = ET.parse(\n",
    "    \"/Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainlit/experiments/deisseroth/sert_exp.xml\"\n",
    ")\n",
    "root = tree.getroot()\n",
    "root.tag\n",
    "for child in root:\n",
    "    for i, entry in enumerate(child):\n",
    "        for item in entry:\n",
    "            if item.tag == \"structure-id\":\n",
    "                region = int(item.text)\n",
    "            elif item.tag == \"hemisphere-id\":\n",
    "                hemi = int(item.text)\n",
    "            elif item.tag == \"is-injection\":\n",
    "                inject = item.text\n",
    "            elif item.tag == \"projection-density\":\n",
    "                density = float(item.text)\n",
    "        if region in subregions_list and hemi == 3 and inject == \"false\":\n",
    "            name = G.nodes[region][\"name\"]\n",
    "            print(f\"id: {region} hemi: {hemi}, density: {density}, name: {name}\")\n",
    "            subregion_name.append(name)\n",
    "            gene.append(\"Allen\")\n",
    "            axon_denss.append(density)\n",
    "\n",
    "\n",
    "d = {\"Axon Density\": axon_denss, \"Gene\": gene, \"Subregion\": subregion_name}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20, 10))\n",
    "fig.suptitle(\"Detected Output Axons\")\n",
    "\n",
    "sns.barplot(x=\"Axon Density\", y=\"Subregion\", hue=\"Gene\", data=df)\n",
    "axes.set_title(\"Density\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_denss = []\n",
    "axon_vols = []\n",
    "gene = []\n",
    "region_name = []\n",
    "for region in allen_regions:\n",
    "    print(f\"Populating: \" + G.nodes[region][\"name\"])\n",
    "    for brain in quantification_dicts.keys():\n",
    "        if (\n",
    "            G.nodes[region][brain + \" total\"] == 0\n",
    "            and G.nodes[region][brain + \" axon\"] == 0\n",
    "        ):\n",
    "            axon_denss.append(0)\n",
    "        elif G.nodes[region][brain + \" total\"] == 0:\n",
    "            raise ValueError(\"positive axon volume in zero volume region?\")\n",
    "        else:\n",
    "            axon_denss.append(\n",
    "                G.nodes[region][brain + \" axon\"] / G.nodes[region][brain + \" total\"]\n",
    "            )\n",
    "            axon_vols.append(\n",
    "                G.nodes[region][brain + \" axon\"]\n",
    "                * np.product([1.82, 1.82, 2])\n",
    "                * 10 ** (-9)\n",
    "            )\n",
    "\n",
    "        if brain in [\"B\", \"R\"]:\n",
    "            gene.append(\"Sample \" + brain)\n",
    "\n",
    "        region_name.append(G.nodes[region][\"name\"])\n",
    "\n",
    "tree = ET.parse(\n",
    "    \"/Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainlit/experiments/deisseroth/sert_exp.xml\"\n",
    ")\n",
    "root = tree.getroot()\n",
    "root.tag\n",
    "for child in root:\n",
    "    for i, entry in enumerate(child):\n",
    "        for item in entry:\n",
    "            if item.tag == \"structure-id\":\n",
    "                region = int(item.text)\n",
    "            elif item.tag == \"hemisphere-id\":\n",
    "                hemi = int(item.text)\n",
    "            elif item.tag == \"is-injection\":\n",
    "                inject = item.text\n",
    "            elif item.tag == \"projection-density\":\n",
    "                density = float(item.text)\n",
    "            elif item.tag == \"projection-volume\":\n",
    "                volume = float(item.text)\n",
    "        if region in allen_regions and hemi == 3 and inject == \"false\":\n",
    "            name = G.nodes[region][\"name\"]\n",
    "            print(\n",
    "                f\"id: {region} hemi: {hemi}, density: {density}, volume: {volume}, name: {name}\"\n",
    "            )\n",
    "            region_name.append(name)\n",
    "            gene.append(\"Allen\")\n",
    "            axon_denss.append(density)\n",
    "            axon_vols.append(volume)\n",
    "\n",
    "\n",
    "d = {\n",
    "    \"Axon Density\": axon_denss,\n",
    "    \"Axon Volume ($mm^3$)\": axon_vols,\n",
    "    \"Gene\": gene,\n",
    "    \"Region\": region_name,\n",
    "}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "fig.suptitle(\"Comparing Axon Volumes to Allen Experiment\")\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "sns.barplot(\n",
    "    ax=axes[0],\n",
    "    x=\"Axon Density\",\n",
    "    y=\"Region\",\n",
    "    hue=\"Gene\",\n",
    "    order=list(\n",
    "        df[df[\"Gene\"] == \"Allen\"]\n",
    "        .sort_values(\"Axon Density\", ascending=False)\n",
    "        .loc[:, \"Region\"]\n",
    "    ),\n",
    "    data=df,\n",
    ")\n",
    "# axes[0].set_title(\"Density\")\n",
    "\n",
    "sns.barplot(\n",
    "    ax=axes[1],\n",
    "    x=\"Axon Volume ($mm^3$)\",\n",
    "    y=\"Region\",\n",
    "    hue=\"Gene\",\n",
    "    order=list(\n",
    "        df[df[\"Gene\"] == \"Allen\"]\n",
    "        .sort_values(\"Axon Density\", ascending=False)\n",
    "        .loc[:, \"Region\"]\n",
    "    ),\n",
    "    data=df,\n",
    ")\n",
    "# axes[1].set_title(\"Axon Volume\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dc00d68ff54f8375e99934614da4863299fb9e10af4294c095b7f517546ff26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('docs_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
